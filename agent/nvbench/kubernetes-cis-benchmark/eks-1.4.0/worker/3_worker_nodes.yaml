version: eks-1.4.0
id: 3
title: 3 - Worker Nodes
type: node
groups:
  - id: 3.1
    title: 3.1 Worker Node Configuration Files
    checks:
      - id: K.3.1.1
        description: Ensure that the kubeconfig file permissions are set to 644 or
          more restrictive (Manual)
        type: worker
        category: kubernetes
        scored: false
        profile: Level 1
        automated: true
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          file="/var/lib/kubelet/kubeconfig"
          file=$(append_prefix "$CONFIG_PREFIX" "$file")
          if [ -f "$file" ]; then
            if [ "$(stat -c %a $file)" -eq 644 -o "$(stat -c %a $file)" -eq 600 -o "$(stat -c %a $file)" -eq 400 ]; then
              pass "$check"
              pass "      * read check_argument for $file"
            else
              warn "$check"
              warn "      * Wrong permissions for $file"
            fi
          else
            warn "$check"
            warn "      * The kubeconfig service file, $file not found"
          fi
        remediation: Run the below command (based on the file location on your system)
          on the each worker node. For example, chmod 644 /proc/1/root/var/lib/kubelet/kubeconfig
      - id: K.3.1.2
        description: Ensure that the kubelet kubeconfig file ownership is set to root:root
          (Manual)
        type: worker
        category: kubernetes
        scored: false
        profile: Level 1
        automated: false
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if [ -f "$file" ]; then
            if [ "$(stat -c %u%g $file)" -eq 00 ]; then
              pass "$check"
            else
              warn "$check"
              warn "      * Wrong ownership for $file"
            fi
          else
            warn "$check"
            warn "      * The kubeconfig service file, $file not found"
          fi
        remediation: Run the below command (based on the file location on your system)
          on the each worker node. For example, chown root:root /proc/1/root/var/lib/kubelet/kubeconfig
      - id: K.3.1.3
        description: Ensure that the kubelet configuration file has permissions set
          to 644 or more restrictive (Manual)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: false
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          file=""
          file="/etc/kubernetes/kubelet/kubelet-config.json"
          file=$(append_prefix "$CONFIG_PREFIX" "$file")
          if [ -f "$file" ]; then
            if [ "$(stat -c %a $file)" -eq 644 -o "$(stat -c %a $file)" -eq 600 -o "$(stat -c %a $file)" -eq 400 ]; then
              pass "$check"
              pass "      * read check_argument for $file"
            else
              warn "$check"
              warn "      * Wrong permissions for $file"
            fi
          else
            warn "$check"
            warn "      * kubelet configuration file, $file not found"
          fi        
        remediation: Run the below command (based on the file location on your system)
          on the each worker node. For example, chmod chmod 644 /proc/1/root/etc/kubernetes/kubelet/kubelet-config.json
      - id: K.3.1.4
        description: Ensure that the kubelet configuration file ownership is set to
          root:root (Manual)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: false
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"           
          if [ -f "$file" ]; then
            if [ "$(stat -c %u%g $file)" -eq 00 ]; then
              pass "$check"
            else
              warn "$check"
              warn "      * Wrong ownership for $file"
            fi
          else
            warn "$check"
            warn "      * kubelet configuration file, $file not found"
          fi        
        remediation: Run the below command (based on the file location on your system)
          on the each worker node. For example, chown root:root /proc/1/root/etc/kubernetes/kubelet/kubelet-config.json
  - id: 3.2
    title: 3.2 - Kubelet
    checks:
      - id: K.3.2.1
        description: Ensure that the Anonymous Auth is Not Enabled (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: true
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--anonymous-auth=false' >/dev/null 2>&1; then
              pass "$check"
          else
              warn "$check"
          fi
        remediation: |
          Remediation Method 1:
          If configuring via the Kubelet config file, you first need to locate the file.
          To do this, SSH to each node and execute the following command to find the kubelet process:
          ps -ef | grep kubelet
          The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:
          sudo less /path/to/kubelet-config.json
          Disable Anonymous Authentication by setting the following parameter:
          "authentication": { "anonymous": { "enabled": false } }
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.
          For systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:
          --anonymous-auth=false
          For Both Remediation Steps:
          Based on your system, restart the kubelet service and check the service status.
          The following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l                
      - id: K.3.2.2
        description: Ensure that the --authorization-mode argument is not set to AlwaysAllow
          (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: true
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--authorization-mode=AlwaysAllow' >/dev/null 2>&1; then
              warn "$check"
          else
              pass "$check"
          fi
        remediation: |
          Remediation Method 1:
          If configuring via the Kubelet config file, you first need to locate the file.
          To do this, SSH to each node and execute the following command to find the kubelet process:
          ps -ef | grep kubelet
          The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:
          sudo less /path/to/kubelet-config.json
          Enable Webhook Authentication by setting the following parameter:
          "authentication": { "webhook": { "enabled": true } }
          Next, set the Authorization Mode to Webhook by setting the following parameter:
          "authorization": { "mode": "Webhook }
          Finer detail of the authentication and authorization fields can be found in the Kubelet Configuration documentation.
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.
          For systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:
          For Both Remediation Steps:
          Based on your system, restart the kubelet service and check the service status.
          The following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l
      - id: K.3.2.3
        description: Ensure that a Client CA File is Configured (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: true
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--client-ca-file' >/dev/null 2>&1; then
              cafile=$(get_argument_value "$CIS_KUBELET_CMD" '--client-ca-file')
              cafile=$(append_prefix "$CONFIG_PREFIX" "$cafile")
              pass "$check"
              pass "      * client-ca-file: $cafile"
          else
              warn "$check"
          fi        
        remediation: |
          Remediation Method 1:
          If configuring via the Kubelet config file, you first need to locate the file.
          To do this, SSH to each node and execute the following command to find the kubelet process:
          ps -ef | grep kubelet
          The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:
          sudo less /path/to/kubelet-config.json
          Configure the client certificate authority file by setting the following parameter appropriately:
          "authentication": { "x509": {"clientCAFile": <path/to/client-ca-file> } }"
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.
          For systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:
          --client-ca-file=<path/to/client-ca-file>
          For Both Remediation Steps:
          Based on your system, restart the kubelet service and check the service status.
          The following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:
      - id: K.3.2.4
        description: Ensure that the --read-only-port is disabled (Manual)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: false
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--read-only-port' >/dev/null 2>&1; then
              port=$(get_argument_value "$CIS_KUBELET_CMD" '--read-only-port' | cut -d "=" -f 2)
              if [ $port = "0" ]; then
                  pass "$check"
              else
                  warn "$check"
                  warn "      * read-only-port: $port"
              fi
          else
              warn "$check"
          fi        
        remediation: |
          If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to 0
          "readOnlyPort": 0
          If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.
          --read-only-port=0
          For each remediation:
          Based on your system, restart the kubelet service and check status
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l
      - id: K.3.2.5
        description: Ensure that the --streaming-connection-idle-timeout argument
          is not set to 0 (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: false
        tags: {}
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--streaming-connection-idle-timeout=0' >/dev/null 2>&1; then
              timeout=$(get_argument_value "$CIS_KUBELET_CMD" '--streaming-connection-idle-timeout')
              warn "$check"
              warn "      * streaming-connection-idle-timeout: $timeout"
          else
              pass "$check"
          fi        
        remediation: |
          Remediation Method 1:
          If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to a non- zero value in the format of #h#m#s
          "streamingConnectionIdleTimeout": "4h0m0s"
          You should ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not specify a -- streaming-connection-idle-timeout argument because it would override the Kubelet config file.
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.
          --streaming-connection-idle-timeout=4h0m0s
          Remediation Method 3:
          If using the api configz endpoint consider searching for the status of "streamingConnectionIdleTimeout": by extracting the live configuration from the nodes running kubelet.
          **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes
          kubectl proxy --port=8001 &
          export HOSTNAME_PORT=localhost:8001 (example host and port number)
          export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from
          "kubectl get nodes")
          curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
          For all three remediations:
          Based on your system, restart the kubelet service and check status
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l
      - id: K.3.2.6
        description: Ensure that the --make-iptables-util-chains argument is set to
          true (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: true
        tags: {}
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--make-iptables-util-chains=true' >/dev/null 2>&1; then
              pass "$check"
          else
              warn "$check"
          fi        
        remediation: |
          Remediation Method 1:
          If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true
          "makeIPTablesUtilChains": true
          Ensure that /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --make-iptables-util-chains argument because that would override your Kubelet config file.
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.
          --make-iptables-util-chains:true
          Remediation Method 3:
          If using the api configz endpoint consider searching for the status of "makeIPTablesUtilChains.: true by extracting the live configuration from the nodes running kubelet.
          **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes
          kubectl proxy --port=8001 &
          export HOSTNAME_PORT=localhost:8001 (example host and port number)
          export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from
          "kubectl get nodes")
          curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
          For all three remediations:
          Based on your system, restart the kubelet service and check status
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l
      - id: K.3.2.7
        description: Ensure that the --eventRecordQPS argument is set to 0 or a level
          which ensures appropriate event capture (Automated)
        type: worker
        category: kubernetes
        scored: false
        profile: Level 2
        automated: false
        tags: {}
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--event-qps' >/dev/null 2>&1; then
              qps=$(get_argument_value "$CIS_KUBELET_CMD" '--event-qps' | cut -d " " -f 1)
              if [ "$qps" -gt 0 ]; then
                  pass "$check"
              else
                  warn "$check"
                  warn "      * --event-qps: $qps is not set to a positive value"
              fi
          else
              warn "$check"
              warn "      * --event-qps is not set"
          fi     
        remediation: |
          If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.
          Based on your system, restart the kubelet service. For example:
          Based on your system, restart the kubelet service and check status
          systemctl daemon-reload
          systemctl restart kubelet.service
      - id: K.3.2.8
        description: Ensure that the --rotate-certificates argument is not present
          or is set to true (Manual)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 2
        automated: true
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          if check_argument "$CIS_KUBELET_CMD" '--rotate-certificates=false' >/dev/null 2>&1; then
            warn "$check"
          else
            pass "$check"
          fi   
        remediation: |
          Remediation Method 1:
          If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true
          "RotateCertificate":true
          Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the -- RotateCertificate executable argument to false because this would override the Kubelet config file.
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.
          --RotateCertificate=true
      - id: K.3.2.9
        description: Ensure that the RotateKubeletServerCertificate argument is set
          to true (Automated)
        type: worker
        category: kubernetes
        scored: true
        profile: Level 1
        automated: false
        tags:
          HIPAA: []
          PCI: []
          GDPR: []
        audit: |
          check="$id  - $description"
          file=""
          if check_argument "$CIS_KUBELET_CMD" '--config' >/dev/null 2>&1; then
              file=$(get_argument_value "$CIS_KUBELET_CMD" '--config'|cut -d " " -f 1)
          fi
          file=$(append_prefix "$CONFIG_PREFIX" "$file")
          if [ -f "$file" ]; then
            found=$(sed -rn '/--feature-gates=RotateKubeletServerCertificate=true/p' $file)
            if [ -z "$found" ]; then
                warn "$check"
                warn "      * --feature-gates=RotateKubeletServerCertificate=true not found"
            else
                pass "$check"
            fi
          else
            warn "$check"
            warn "      * The kubeconfig service file not found"
          fi
        remediation: |
          Remediation Method 1:
          If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true
          "featureGates": {
            "RotateKubeletServerCertificate":true
          \}
          Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the -- rotate-kubelet-server-certificate executable argument to false because this would override the Kubelet config file.
          Remediation Method 2:
          If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.
          --rotate-kubelet-server-certificate=true
          Remediation Method 3:
          If using the api configz endpoint consider searching for the status of "RotateKubeletServerCertificate": by extracting the live configuration from the nodes running kubelet.
          **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes
          kubectl proxy --port=8001 &
          export HOSTNAME_PORT=localhost:8001 (example host and port number)
          export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from
          "kubectl get nodes")
          curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
          For all three remediation methods:
          Restart the kubelet service and check status. The example below is for when using systemctl to manage services:
          systemctl daemon-reload
          systemctl restart kubelet.service
          systemctl status kubelet -l
  - id: 3.3
    title: 3.3 - Container Optimized OS
    checks:
      - id: K.3.3.1
        description: Prefer using a container-optimized OS when possible (Manual)
        type: worker
        category: kubernetes
        scored: false
        profile: Level 2
        automated: true
        tags: {}
        audit: |
          check="$id  - $description"
          manual "$check"
          manual "      * If a container-optimized OS is required examine the nodes in EC2 and click on their AMI to ensure that it is a container-optimized OS like Amazon Bottlerocket; or connect to the worker node and check its OS. "
        remediation: None
